version: 2.1

jobs:
  unit:
    environment:
      DBT_INVOCATION_ENV: circle
    docker:
      - image: fishtownanalytics/test-container:10
    steps:
      - checkout
      - run: tox -e flake8,unit

  integration-spark-session:
    environment:
      DBT_INVOCATION_ENV: circle
    docker:
      - image: godatadriven/pyspark:3.1
    steps:
      - checkout
      - run: apt-get update
      - run: python3 -m pip install --upgrade pip
      - run: apt-get install -y git gcc g++ unixodbc-dev libsasl2-dev
      - run: python3 -m pip install tox
      - run:
          name: Run integration tests
          command: tox -e integration-spark-session
          no_output_timeout: 1h
      - store_artifacts:
          path: ./logs

  integration-spark-thrift:
    environment:
      DBT_INVOCATION_ENV: circle
    docker:
      - image: fishtownanalytics/test-container:10
      - image: godatadriven/spark:2
        environment:
          WAIT_FOR: localhost:5432
        command: >
          --class org.apache.spark.sql.hive.thriftserver.HiveThriftServer2
          --name Thrift JDBC/ODBC Server
          --conf spark.hadoop.javax.jdo.option.ConnectionURL=jdbc:postgresql://localhost/metastore
          --conf spark.hadoop.javax.jdo.option.ConnectionUserName=dbt
          --conf spark.hadoop.javax.jdo.option.ConnectionPassword=dbt
          --conf spark.hadoop.javax.jdo.option.ConnectionDriverName=org.postgresql.Driver
          --conf spark.serializer=org.apache.spark.serializer.KryoSerializer
          --conf spark.jars.packages=org.apache.hudi:hudi-spark-bundle_2.11:0.9.0
          --conf spark.sql.extensions=org.apache.spark.sql.hudi.HoodieSparkSessionExtension
          --conf spark.driver.userClassPathFirst=true
          --conf spark.hadoop.datanucleus.autoCreateTables=true
          --conf spark.hadoop.datanucleus.schema.autoCreateTables=true
          --conf spark.hadoop.datanucleus.fixedDatastore=false
          --conf spark.sql.hive.convertMetastoreParquet=false
          --hiveconf hoodie.datasource.hive_sync.use_jdbc=false
          --hiveconf hoodie.datasource.hive_sync.mode=hms
          --hiveconf datanucleus.schema.autoCreateAll=true
          --hiveconf hive.metastore.schema.verification=false

      - image: postgres:9.6.17-alpine
        environment:
          POSTGRES_USER: dbt
          POSTGRES_PASSWORD: dbt
          POSTGRES_DB: metastore

    steps:
      - checkout

      - run:
          name: Wait for Spark-Thrift
          command: dockerize -wait tcp://localhost:10000 -timeout 15m -wait-retry-interval 5s

      - run:
          name: Run integration tests
          command: tox -e integration-spark-thrift
          no_output_timeout: 1h
      - store_artifacts:
          path: ./logs

  integration-spark-databricks-http:
    environment:
      DBT_INVOCATION_ENV: circle
      DBT_DATABRICKS_RETRY_ALL: True
    docker:
      - image: fishtownanalytics/test-container:10
    steps:
      - checkout
      - run:
          name: Run integration tests
          command: tox -e integration-spark-databricks-http
          no_output_timeout: 1h
      - store_artifacts:
          path: ./logs

  integration-spark-databricks-odbc-cluster: &databricks-odbc
    environment:
      DBT_INVOCATION_ENV: circle
      ODBC_DRIVER: Simba # TODO: move env var to Docker image
    docker:
      # image based on `fishtownanalytics/test-container` w/ Simba ODBC Spark driver installed
      - image: 828731156495.dkr.ecr.us-east-1.amazonaws.com/dbt-spark-odbc-test-container:latest
        aws_auth:
          aws_access_key_id: $AWS_ACCESS_KEY_ID_STAGING
          aws_secret_access_key: $AWS_SECRET_ACCESS_KEY_STAGING
    steps:
      - checkout
      - run:
          name: Run integration tests
          command: tox -e integration-spark-databricks-odbc-cluster
          no_output_timeout: 1h
      - store_artifacts:
          path: ./logs

  integration-spark-databricks-odbc-endpoint:
    <<: *databricks-odbc
    steps:
      - checkout
      - run:
          name: Run integration tests
          command: tox -e integration-spark-databricks-odbc-sql-endpoint
          no_output_timeout: 1h
      - store_artifacts:
          path: ./logs

workflows:
  version: 2
  test-everything:
    jobs:
      - unit
      - integration-spark-session:
          requires:
            - unit
      - integration-spark-thrift:
          requires:
            - unit
      - integration-spark-databricks-http:
          requires:
            - integration-spark-thrift
      - integration-spark-databricks-odbc-cluster:
          requires:
            - integration-spark-thrift
      - integration-spark-databricks-odbc-endpoint:
          requires:
            - integration-spark-thrift
